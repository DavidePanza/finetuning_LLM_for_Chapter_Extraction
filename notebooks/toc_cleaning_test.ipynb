{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6a90178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdc857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "1\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "1.1\n",
      "What this book is all about . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "1.2\n",
      "What is vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "3\n",
      "1.3\n",
      "The magic of your visual system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "3\n",
      "1.4\n",
      "Importance of prior information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "1.4.1\n",
      "Ecological adaptation provides prior information . . . . . . . . . .\n",
      "6\n",
      "1.4.2\n",
      "Generative models and latent quantities . . . . . . . . . . . . . . . . . .\n",
      "8\n",
      "1.4.3\n",
      "Projection onto the retina loses information . . . . . . . . . . . . . .\n",
      "9\n",
      "1.4.4\n",
      "Bayesian inference and priors . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
      "1.5\n",
      "Natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "1.5.1\n",
      "The image space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "1.5.2\n",
      "Deﬁnition of natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "1.6\n",
      "Redundancy and information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "1.6.1\n",
      "Information theory and image coding . . . . . . . . . . . . . . . . . . . 13\n",
      "1.6.2\n",
      "Redundancy reduction and neural coding . . . . . . . . . . . . . . . . 15\n",
      "1.7\n",
      "Statistical modelling of the visual system . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "1.7.1\n",
      "Connecting information theory and Bayesian inference. . . . . 16\n",
      "1.7.2\n",
      "Normative vs. descriptive modelling of visual system . . . . . . 16\n",
      "1.7.3\n",
      "Towards predictive theoretical neuroscience . . . . . . . . . . . . . . 17\n",
      "1.8\n",
      "Features and statistical models of natural images . . . . . . . . . . . . . . . . 18\n",
      "1.8.1\n",
      "Image representations and features. . . . . . . . . . . . . . . . . . . . . . 18\n",
      "1.8.2\n",
      "Statistics of features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "1.8.3\n",
      "From features to statistical models . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "1.9\n",
      "The statistical-ecological approach recapitulated. . . . . . . . . . . . . . . . . 21\n",
      "1.10 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "Part I Background\n",
      "2\n",
      "Linear ﬁlters and frequency analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "2.1\n",
      "Linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "2.1.1\n",
      "Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "vii\n",
      "\n",
      "viii\n",
      "Contents\n",
      "2.1.2\n",
      "Impulse response and convolution . . . . . . . . . . . . . . . . . . . . . . 28\n",
      "2.2\n",
      "Frequency-based representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "2.2.1\n",
      "Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "2.2.2\n",
      "Representation in one and two dimensions . . . . . . . . . . . . . . . 30\n",
      "2.2.3\n",
      "Frequency-based representation and linear ﬁltering . . . . . . . . 33\n",
      "2.2.4\n",
      "Computation and mathematical details . . . . . . . . . . . . . . . . . . 37\n",
      "2.3\n",
      "Representation using linear basis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n",
      "2.3.1\n",
      "Basic idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n",
      "2.3.2\n",
      "Frequency-based representation as a basis. . . . . . . . . . . . . . . . 40\n",
      "2.4\n",
      "Space-frequency analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "2.4.1\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "2.4.2\n",
      "Space-frequency analysis and Gabor ﬁlters . . . . . . . . . . . . . . . 43\n",
      "2.4.3\n",
      "Spatial localization vs. spectral accuracy . . . . . . . . . . . . . . . . . 46\n",
      "2.5\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n",
      "2.6\n",
      "Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n",
      "3\n",
      "Outline of the visual system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n",
      "3.1\n",
      "Neurons and ﬁring rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n",
      "3.2\n",
      "From the eye to the cortex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n",
      "3.3\n",
      "Linear models of visual neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n",
      "3.3.1\n",
      "Responses to visual stimulation . . . . . . . . . . . . . . . . . . . . . . . . 55\n",
      "3.3.2\n",
      "Simple cells and linear models . . . . . . . . . . . . . . . . . . . . . . . . . 56\n",
      "3.3.3\n",
      "Gabor models and selectivities of simple cells . . . . . . . . . . . . 58\n",
      "3.3.4\n",
      "Frequency channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n",
      "3.4\n",
      "Nonlinear models of visual neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n",
      "3.4.1\n",
      "Nonlinearities in simple-cell responses . . . . . . . . . . . . . . . . . . 59\n",
      "3.4.2\n",
      "Complex cells and energy models . . . . . . . . . . . . . . . . . . . . . . 62\n",
      "3.5\n",
      "Interactions between visual neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n",
      "3.6\n",
      "Topographic organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n",
      "3.7\n",
      "Processing after the primary visual cortex . . . . . . . . . . . . . . . . . . . . . . 66\n",
      "3.8\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n",
      "3.9\n",
      "Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n",
      "4\n",
      "Multivariate probability and statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n",
      "4.1\n",
      "Natural images patches as random vectors . . . . . . . . . . . . . . . . . . . . . . 69\n",
      "4.2\n",
      "Multivariate probability distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n",
      "4.2.1\n",
      "Notation and motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n",
      "4.2.2\n",
      "Probability density function . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n",
      "4.3\n",
      "Marginal and joint probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n",
      "4.4\n",
      "Conditional probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n",
      "4.5\n",
      "Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n",
      "4.6\n",
      "Expectation and covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n",
      "4.6.1\n",
      "Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n",
      "4.6.2\n",
      "Variance and covariance in one dimension . . . . . . . . . . . . . . . 80\n",
      "4.6.3\n",
      "Covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n",
      "\n",
      "Contents\n",
      "ix\n",
      "4.6.4\n",
      "Independence and covariances . . . . . . . . . . . . . . . . . . . . . . . . . 81\n",
      "4.7\n",
      "Bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n",
      "4.7.1\n",
      "Motivating example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n",
      "4.7.2\n",
      "Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n",
      "4.7.3\n",
      "Non-informative priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n",
      "4.7.4\n",
      "Bayesian inference as an incremental learning process . . . . . 86\n",
      "4.8\n",
      "Parameter estimation and likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "4.8.1\n",
      "Models, estimation, and samples . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "4.8.2\n",
      "Maximum likelihood and maximum a posteriori . . . . . . . . . . 89\n",
      "4.8.3\n",
      "Prior and large samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n",
      "4.9\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "4.10 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "Part II Statistics of linear features\n",
      "5\n",
      "Principal components and whitening . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n",
      "5.1\n",
      "DC component or mean grey-scale value . . . . . . . . . . . . . . . . . . . . . . . 97\n",
      "5.2\n",
      "Principal component analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n",
      "5.2.1\n",
      "A basic dependency of pixels in natural images . . . . . . . . . . . 99\n",
      "5.2.2\n",
      "Learning one feature by maximization of variance . . . . . . . . . 100\n",
      "5.2.3\n",
      "Learning many features by PCA . . . . . . . . . . . . . . . . . . . . . . . . 103\n",
      "5.2.4\n",
      "Computational implementation of PCA . . . . . . . . . . . . . . . . . . 104\n",
      "5.2.5\n",
      "The implications of translation-invariance . . . . . . . . . . . . . . . . 106\n",
      "5.3\n",
      "PCA as a preprocessing tool. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n",
      "5.3.1\n",
      "Dimension reduction by PCA . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n",
      "5.3.2\n",
      "Whitening by PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n",
      "5.3.3\n",
      "Anti-aliasing by PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n",
      "5.4\n",
      "Canonical preprocessing used in this book . . . . . . . . . . . . . . . . . . . . . . 113\n",
      "5.5\n",
      "Gaussianity as the basis for PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "5.5.1\n",
      "The probability model related to PCA . . . . . . . . . . . . . . . . . . . 115\n",
      "5.5.2\n",
      "PCA as a generative model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "5.5.3\n",
      "Image synthesis results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n",
      "5.6\n",
      "Power spectrum of natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n",
      "5.6.1\n",
      "The 1/ f Fourier amplitude or 1/ f 2 power spectrum . . . . . . . 116\n",
      "5.6.2\n",
      "Connection between power spectrum and covariances . . . . . . 119\n",
      "5.6.3\n",
      "Relative importance of amplitude and phase . . . . . . . . . . . . . . 120\n",
      "5.7\n",
      "Anisotropy in natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n",
      "5.8\n",
      "Mathematics of principal component analysis* . . . . . . . . . . . . . . . . . . 122\n",
      "5.8.1\n",
      "Eigenvalue decomposition of the covariance matrix . . . . . . . . 123\n",
      "5.8.2\n",
      "Eigenvectors and translation-invariance . . . . . . . . . . . . . . . . . . 125\n",
      "5.9\n",
      "Decorrelation models of retina and LGN *. . . . . . . . . . . . . . . . . . . . . . 126\n",
      "5.9.1\n",
      "Whitening and redundancy reduction . . . . . . . . . . . . . . . . . . . . 126\n",
      "5.9.2\n",
      "Patch-based decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n",
      "5.9.3\n",
      "Filter-based decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n",
      "5.10 Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n",
      "\n",
      "x\n",
      "Contents\n",
      "5.11 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "6\n",
      "Sparse coding and simple cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n",
      "6.1\n",
      "Deﬁnition of sparseness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n",
      "6.2\n",
      "Learning one feature by maximization of sparseness . . . . . . . . . . . . . 138\n",
      "6.2.1\n",
      "Measuring sparseness: General framework . . . . . . . . . . . . . . . 139\n",
      "6.2.2\n",
      "Measuring sparseness using kurtosis . . . . . . . . . . . . . . . . . . . . 140\n",
      "6.2.3\n",
      "Measuring sparseness using convex functions of square . . . . 141\n",
      "6.2.4\n",
      "The case of canonically preprocessed data . . . . . . . . . . . . . . . 144\n",
      "6.2.5\n",
      "One feature learned from natural images . . . . . . . . . . . . . . . . . 145\n",
      "6.3\n",
      "Learning many features by maximization of sparseness . . . . . . . . . . . 145\n",
      "6.3.1\n",
      "Deﬂationary decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n",
      "6.3.2\n",
      "Symmetric decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n",
      "6.3.3\n",
      "Sparseness of feature vs. sparseness of representation . . . . . . 148\n",
      "6.4\n",
      "Sparse coding features for natural images . . . . . . . . . . . . . . . . . . . . . . 150\n",
      "6.4.1\n",
      "Full set of features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n",
      "6.4.2\n",
      "Analysis of tuning properties . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n",
      "6.5\n",
      "How is sparseness useful? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n",
      "6.5.1\n",
      "Bayesian modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n",
      "6.5.2\n",
      "Neural modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n",
      "6.5.3\n",
      "Metabolic economy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n",
      "6.6\n",
      "Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n",
      "6.7\n",
      "Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n",
      "7\n",
      "Independent component analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "7.1\n",
      "Limitations of the sparse coding approach . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "7.2\n",
      "Deﬁnition of ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "7.2.1\n",
      "Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "7.2.2\n",
      "Generative model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n",
      "7.2.3\n",
      "Model for preprocessed data . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n",
      "7.3\n",
      "Insufﬁciency of second-order information . . . . . . . . . . . . . . . . . . . . . . 162\n",
      "7.3.1\n",
      "Why whitening does not ﬁnd independent components . . . . . 163\n",
      "7.3.2\n",
      "Why components have to be non-gaussian . . . . . . . . . . . . . . . 164\n",
      "7.4\n",
      "The probability density deﬁned by ICA . . . . . . . . . . . . . . . . . . . . . . . . 166\n",
      "7.5\n",
      "Maximum likelihood estimation in ICA . . . . . . . . . . . . . . . . . . . . . . . . 167\n",
      "7.6\n",
      "Results on natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n",
      "7.6.1\n",
      "Estimation of features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n",
      "7.6.2\n",
      "Image synthesis using ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n",
      "7.7\n",
      "Connection to maximization of sparseness . . . . . . . . . . . . . . . . . . . . . . 170\n",
      "7.7.1\n",
      "Likelihood as a measure of sparseness . . . . . . . . . . . . . . . . . . . 171\n",
      "7.7.2\n",
      "Optimal sparseness measures . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n",
      "7.8\n",
      "Why are independent components sparse? . . . . . . . . . . . . . . . . . . . . . . 175\n",
      "7.8.1\n",
      "Different forms of non-gaussianity . . . . . . . . . . . . . . . . . . . . . . 175\n",
      "7.8.2\n",
      "Non-gaussianity in natural images . . . . . . . . . . . . . . . . . . . . . . 176\n",
      "7.8.3\n",
      "Why is sparseness dominant? . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n",
      "\n",
      "Contents\n",
      "xi\n",
      "7.9\n",
      "General ICA as maximization of non-gaussianity . . . . . . . . . . . . . . . . 177\n",
      "7.9.1\n",
      "Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n",
      "7.9.2\n",
      "“Non-gaussian is independent” . . . . . . . . . . . . . . . . . . . . . . . . . 178\n",
      "7.9.3\n",
      "Sparse coding as a special case of ICA . . . . . . . . . . . . . . . . . . 179\n",
      "7.10 Receptive ﬁelds vs. feature vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n",
      "7.11 Problem of inversion of preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "7.12 Frequency channels and ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n",
      "7.13 Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n",
      "7.14 Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n",
      "8\n",
      "Information-theoretic interpretations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n",
      "8.1\n",
      "Basic motivation for information theory . . . . . . . . . . . . . . . . . . . . . . . . 185\n",
      "8.1.1\n",
      "Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n",
      "8.1.2\n",
      "Transmission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n",
      "8.2\n",
      "Entropy as a measure of uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n",
      "8.2.1\n",
      "Deﬁnition of entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n",
      "8.2.2\n",
      "Entropy as minimum coding length . . . . . . . . . . . . . . . . . . . . . 189\n",
      "8.2.3\n",
      "Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n",
      "8.2.4\n",
      "Differential entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "8.2.5\n",
      "Maximum entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n",
      "8.3\n",
      "Mutual information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n",
      "8.4\n",
      "Minimum entropy coding of natural images. . . . . . . . . . . . . . . . . . . . . 194\n",
      "8.4.1\n",
      "Image compression and sparse coding . . . . . . . . . . . . . . . . . . . 194\n",
      "8.4.2\n",
      "Mutual information and sparse coding . . . . . . . . . . . . . . . . . . . 195\n",
      "8.4.3\n",
      "Minimum entropy coding in the cortex . . . . . . . . . . . . . . . . . . 196\n",
      "8.5\n",
      "Information transmission in the nervous system . . . . . . . . . . . . . . . . . 196\n",
      "8.5.1\n",
      "Deﬁnition of information ﬂow and infomax . . . . . . . . . . . . . . 196\n",
      "8.5.2\n",
      "Basic infomax with linear neurons . . . . . . . . . . . . . . . . . . . . . . 197\n",
      "8.5.3\n",
      "Infomax with nonlinear neurons . . . . . . . . . . . . . . . . . . . . . . . . 198\n",
      "8.5.4\n",
      "Infomax with non-constant noise variance . . . . . . . . . . . . . . . 199\n",
      "8.6\n",
      "Caveats in application of information theory . . . . . . . . . . . . . . . . . . . . 202\n",
      "8.7\n",
      "Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n",
      "8.8\n",
      "Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n",
      "Part III Nonlinear features & dependency of linear features\n",
      "9\n",
      "Energy correlation of linear features & normalization . . . . . . . . . . . . . . 209\n",
      "9.1\n",
      "Why estimated independent components are not independent . . . . . . 209\n",
      "9.1.1\n",
      "Estimates vs. theoretical components . . . . . . . . . . . . . . . . . . . . 209\n",
      "9.1.2\n",
      "Counting the number of free parameters . . . . . . . . . . . . . . . . . 211\n",
      "9.2\n",
      "Correlations of squares of components in natural images . . . . . . . . . . 211\n",
      "9.3\n",
      "Modelling using a variance variable . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n",
      "9.4\n",
      "Normalization of variance and contrast gain control . . . . . . . . . . . . . . 214\n",
      "9.5\n",
      "Physical and neurophysiological interpretations . . . . . . . . . . . . . . . . . 216\n",
      "9.5.1\n",
      "Cancelling the effect of changing lighting conditions . . . . . . 216\n",
      "\n",
      "xii\n",
      "Contents\n",
      "9.5.2\n",
      "Uniform surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "9.5.3\n",
      "Saturation of cell responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n",
      "9.6\n",
      "Effect of normalization on ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n",
      "9.7\n",
      "Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "9.8\n",
      "Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n",
      "10\n",
      "Energy detectors and complex cells. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n",
      "10.1 Subspace model of invariant features . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n",
      "10.1.1 Why linear features are insufﬁcient . . . . . . . . . . . . . . . . . . . . . 223\n",
      "10.1.2 Subspaces or groups of linear features . . . . . . . . . . . . . . . . . . . 224\n",
      "10.1.3 Energy model of feature detection . . . . . . . . . . . . . . . . . . . . . . 225\n",
      "10.2 Maximizing sparseness in the energy model . . . . . . . . . . . . . . . . . . . . 227\n",
      "10.2.1 Deﬁnition of sparseness of output . . . . . . . . . . . . . . . . . . . . . . 227\n",
      "10.2.2 One feature learned from natural images . . . . . . . . . . . . . . . . . 228\n",
      "10.3 Model of independent subspace analysis . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "10.4 Dependency as energy correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\n",
      "10.4.1 Why energy correlations are related to sparseness . . . . . . . . . 231\n",
      "10.4.2 Spherical symmetry and changing variance. . . . . . . . . . . . . . . 231\n",
      "10.4.3 Correlation of squares and convexity of nonlinearity . . . . . . . 232\n",
      "10.5 Connection to contrast gain control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n",
      "10.6 ISA as a nonlinear version of ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n",
      "10.7 Results on natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n",
      "10.7.1 Emergence of invariance to phase. . . . . . . . . . . . . . . . . . . . . . . 235\n",
      "10.7.2 The importance of being invariant . . . . . . . . . . . . . . . . . . . . . . 242\n",
      "10.7.3 Grouping of dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n",
      "10.7.4 Superiority of the model over ICA . . . . . . . . . . . . . . . . . . . . . . 243\n",
      "10.8 Analysis of convexity and energy correlations* . . . . . . . . . . . . . . . . . . 245\n",
      "10.8.1 Variance variable model gives convex h. . . . . . . . . . . . . . . . . . 246\n",
      "10.8.2 Convex h typically implies positive energy correlations . . . . 246\n",
      "10.9 Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n",
      "10.10Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n",
      "11\n",
      "Energy correlations and topographic organization . . . . . . . . . . . . . . . . . 249\n",
      "11.1 Topography in the cortex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n",
      "11.2 Modelling topography by statistical dependence . . . . . . . . . . . . . . . . . 250\n",
      "11.2.1 Topographic grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n",
      "11.2.2 Deﬁning topography by statistical dependencies . . . . . . . . . . 251\n",
      "11.3 Deﬁnition of topographic ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n",
      "11.4 Connection to independent subspaces and invariant features . . . . . . . 254\n",
      "11.5 Utility of topography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n",
      "11.6 Estimation of topographic ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n",
      "11.7 Topographic ICA of natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "11.7.1 Emergence of V1-like topography . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "11.7.2 Comparison with other models . . . . . . . . . . . . . . . . . . . . . . . . . 262\n",
      "11.8 Learning both layers in a two-layer model * . . . . . . . . . . . . . . . . . . . . 264\n",
      "\n",
      "Contents\n",
      "xiii\n",
      "11.8.1 Generative vs. energy-based approach . . . . . . . . . . . . . . . . . . . 264\n",
      "11.8.2 Deﬁnition of the generative model . . . . . . . . . . . . . . . . . . . . . . 264\n",
      "11.8.3 Basic properties of the generative model . . . . . . . . . . . . . . . . . 265\n",
      "11.8.4 Estimation of the generative model . . . . . . . . . . . . . . . . . . . . . 266\n",
      "11.8.5 Energy-based two-layer models . . . . . . . . . . . . . . . . . . . . . . . . 270\n",
      "11.9 Concluding remarks and References . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n",
      "12\n",
      "Dependencies of energy detectors: Beyond V1 . . . . . . . . . . . . . . . . . . . . . 273\n",
      "12.1 Predictive modelling of extrastriate cortex . . . . . . . . . . . . . . . . . . . . . . 273\n",
      "12.2 Simulation of V1 by a ﬁxed two-layer model . . . . . . . . . . . . . . . . . . . 274\n",
      "12.3 Learning the third layer by another ICA model . . . . . . . . . . . . . . . . . . 276\n",
      "12.4 Methods for analysing higher-order components . . . . . . . . . . . . . . . . . 277\n",
      "12.5 Results on natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n",
      "12.5.1 Emergence of collinear contour units . . . . . . . . . . . . . . . . . . . . 278\n",
      "12.5.2 Emergence of pooling over frequencies . . . . . . . . . . . . . . . . . . 279\n",
      "12.6 Discussion of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n",
      "12.6.1 Why coding of contours?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n",
      "12.6.2 Frequency channels and edges . . . . . . . . . . . . . . . . . . . . . . . . . 285\n",
      "12.6.3 Towards predictive modelling . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n",
      "12.6.4 References and related work . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n",
      "12.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n",
      "13\n",
      "Overcomplete and non-negative models . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n",
      "13.1 Overcomplete bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n",
      "13.1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n",
      "13.1.2 Deﬁnition of generative model . . . . . . . . . . . . . . . . . . . . . . . . . 290\n",
      "13.1.3 Nonlinear computation of the basis coefﬁcients . . . . . . . . . . . 291\n",
      "13.1.4 Estimation of the basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294\n",
      "13.1.5 Approach using energy-based models . . . . . . . . . . . . . . . . . . . 294\n",
      "13.1.6 Results on natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\n",
      "13.1.7 Markov Random Field models * . . . . . . . . . . . . . . . . . . . . . . . . 297\n",
      "13.2 Non-negative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\n",
      "13.2.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\n",
      "13.2.2 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\n",
      "13.2.3 Adding sparseness constraints. . . . . . . . . . . . . . . . . . . . . . . . . . 302\n",
      "13.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n",
      "14\n",
      "Lateral interactions and feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n",
      "14.1 Feedback as Bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n",
      "14.1.1 Example: contour integrator units. . . . . . . . . . . . . . . . . . . . . . . 308\n",
      "14.1.2 Thresholding (shrinkage) of a sparse code . . . . . . . . . . . . . . . 311\n",
      "14.1.3 Categorization and top-down feedback . . . . . . . . . . . . . . . . . . 313\n",
      "14.2 Overcomplete basis and end-stopping . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n",
      "14.3 Predictive coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n",
      "14.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n",
      "\n",
      "xiv\n",
      "Contents\n",
      "Part IV Time, colour and stereo\n",
      "15\n",
      "Colour and stereo images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n",
      "15.1 Colour image experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n",
      "15.1.1 Choice of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\n",
      "15.1.2 Preprocessing and PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "15.1.3 ICA results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "15.2 Stereo image experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n",
      "15.2.1 Choice of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n",
      "15.2.2 Preprocessing and PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n",
      "15.2.3 ICA results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n",
      "15.3 Further references . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "15.3.1 Colour and stereo images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "15.3.2 Other modalities, including audition . . . . . . . . . . . . . . . . . . . . 336\n",
      "15.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n",
      "16\n",
      "Temporal sequences of natural images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "16.1 Natural image sequences and spatiotemporal ﬁltering . . . . . . . . . . . . 337\n",
      "16.2 Temporal and spatiotemporal receptive ﬁelds . . . . . . . . . . . . . . . . . . . 338\n",
      "16.3 Second-order statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n",
      "16.3.1 Average spatiotemporal power spectrum . . . . . . . . . . . . . . . . . 341\n",
      "16.3.2 The temporally decorrelating ﬁlter . . . . . . . . . . . . . . . . . . . . . . 345\n",
      "16.4 Sparse coding and ICA of natural image sequences . . . . . . . . . . . . . . 347\n",
      "16.5 Temporal coherence in spatial features . . . . . . . . . . . . . . . . . . . . . . . . . 349\n",
      "16.5.1 Temporal coherence and invariant representation . . . . . . . . . . 349\n",
      "16.5.2 Quantifying temporal coherence . . . . . . . . . . . . . . . . . . . . . . . . 349\n",
      "16.5.3 Interpretation as generative model *. . . . . . . . . . . . . . . . . . . . . 351\n",
      "16.5.4 Experiments on natural image sequences . . . . . . . . . . . . . . . . 352\n",
      "16.5.5 Why Gabor-like features maximize temporal coherence . . . . 355\n",
      "16.5.6 Control experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n",
      "16.6 Spatiotemporal energy correlations in linear features . . . . . . . . . . . . . 359\n",
      "16.6.1 Deﬁnition of the model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\n",
      "16.6.2 Estimation of the model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360\n",
      "16.6.3 Experiments on natural images . . . . . . . . . . . . . . . . . . . . . . . . . 361\n",
      "16.6.4 Intuitive explanation of results . . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "16.7 Unifying model of spatiotemporal dependencies . . . . . . . . . . . . . . . . . 365\n",
      "16.8 Features with minimal average temporal change . . . . . . . . . . . . . . . . . 367\n",
      "16.8.1 Slow feature analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "16.8.2 Quadratic slow feature analysis . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "16.8.3 Sparse slow feature analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 374\n",
      "16.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\n",
      "Part V Conclusion\n",
      "\n",
      "Contents\n",
      "xv\n",
      "17\n",
      "Conclusion and future prospects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\n",
      "17.1 Short overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\n",
      "17.2 Open, or frequently asked, questions . . . . . . . . . . . . . . . . . . . . . . . . . . 381\n",
      "17.2.1 What is the real learning principle in the brain? . . . . . . . . . . . 382\n",
      "17.2.2 Nature vs. nurture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n",
      "17.2.3 How to model whole images . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n",
      "17.2.4 Are there clear-cut cell types? . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "17.2.5 How far can we go? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n",
      "17.3 Other mathematical models of images . . . . . . . . . . . . . . . . . . . . . . . . . 386\n",
      "17.3.1 Scaling laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\n",
      "17.3.2 Wavelet theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387\n",
      "17.3.3 Physically inspired models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n",
      "17.4 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n",
      "Part VI Appendix: Supplementary mathematical tools\n",
      "18\n",
      "Optimization theory and algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n",
      "18.1 Levels of modelling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n",
      "18.2 Gradient method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\n",
      "18.2.1 Deﬁnition and meaning of gradient . . . . . . . . . . . . . . . . . . . . . 395\n",
      "18.2.2 Gradient and optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\n",
      "18.2.3 Optimization of function of matrix . . . . . . . . . . . . . . . . . . . . . . 397\n",
      "18.2.4 Constrained optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398\n",
      "18.3 Global and local maxima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\n",
      "18.4 Hebb’s rule and gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n",
      "18.4.1 Hebb’s rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n",
      "18.4.2 Hebb’s rule and optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n",
      "18.4.3 Stochastic gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n",
      "18.4.4 Role of the Hebbian nonlinearity . . . . . . . . . . . . . . . . . . . . . . . 403\n",
      "18.4.5 Receptive ﬁelds vs. synaptic strengths . . . . . . . . . . . . . . . . . . . 404\n",
      "18.4.6 The problem of feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n",
      "18.5 Optimization in topographic ICA * . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n",
      "18.6 Beyond basic gradient methods * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\n",
      "18.6.1 Newton’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\n",
      "18.6.2 Conjugate gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\n",
      "18.7 FastICA, a ﬁxed-point algorithm for ICA . . . . . . . . . . . . . . . . . . . . . . . 411\n",
      "18.7.1 The FastICA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n",
      "18.7.2 Choice of the FastICA nonlinearity . . . . . . . . . . . . . . . . . . . . . 411\n",
      "18.7.3 Mathematics of FastICA *. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412\n",
      "19\n",
      "Crash course on linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "19.1 Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "19.2 Linear transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\n",
      "19.3 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "19.4 Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping example keys to PDF paths\n",
    "examples = {\n",
    "    \"pdf_path1\": \"../data/mcelreath_2020_statistical-rethinking.pdf\",\n",
    "    \"pdf_path2\": \"../data/Theory of Statistic.pdf\",\n",
    "    \"pdf_path3\": \"../data/Deep Learning with Python.pdf\",\n",
    "    \"pdf_path4\": \"../data/Natural_Image_Statistics.pdf\",\n",
    "    \"pdf_path5\": \"../data/mml-book.pdf\"\n",
    "}\n",
    "\n",
    "# Dictionary mapping example keys to page ranges to extract content from\n",
    "content_page_ranges = {\n",
    "    \"pdf_path1\": range(5, 8),\n",
    "    \"pdf_path2\": range(10, 17),\n",
    "    \"pdf_path3\": range(7, 13),\n",
    "    \"pdf_path4\": range(4, 13),\n",
    "    \"pdf_path5\": range(2, 5),\n",
    "}\n",
    "\n",
    "# Select example number\n",
    "n_example = 4\n",
    "key = f\"pdf_path{n_example}\"\n",
    "\n",
    "# Open the PDF\n",
    "doc = fitz.open(examples[key])\n",
    "\n",
    "# Extract text from the specified page range\n",
    "chapters_content_list = []\n",
    "for page_num in content_page_ranges[key]:\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text(\"text\")\n",
    "    chapters_content_list.append(text)\n",
    "\n",
    "# Join all text pages into a single string if needed\n",
    "chapters_content = \"\\n\".join(chapters_content_list)\n",
    "\n",
    "print(chapters_content)  # or pass it to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_font_info(pdf_path, header_margin=70, footer_margin=100):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    font_data = []\n",
    "    \n",
    "    for page_num in content_page_ranges[key]:\n",
    "        page = doc.load_page(page_num)\n",
    "        page_height = page.rect.height\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        \n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        y = span[\"origin\"][1]\n",
    "                        # Skip headers/footers using defaults\n",
    "                        if y < header_margin or y > (page_height - footer_margin):\n",
    "                            continue\n",
    "                        font_data.append({\n",
    "                            \"text\": span[\"text\"],\n",
    "                            # \"font_name\": span[\"font\"],\n",
    "                            # \"font_size\": span[\"size\"],\n",
    "                            # \"color\": span[\"color\"],  # RGB tuple (e.g., (0, 0, 0) for black)\n",
    "                            # \"is_bold\": \"bold\" in span[\"font\"].lower(),\n",
    "                            # \"is_italic\": \"italic\" in span[\"font\"].lower(),\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"coordinates\": (span[\"origin\"][0], span[\"origin\"][1])\n",
    "                        })\n",
    "    return font_data\n",
    "\n",
    "\n",
    "def extract_lines_from_font_info(font_info):\n",
    "    \"\"\"\n",
    "    Extracts lines of text from font information based on y-coordinates.\n",
    "    This function assumes that text elements with the same y-coordinate belong to the same line.\n",
    "    \"\"\"\n",
    "    if not font_info:\n",
    "        return []\n",
    "    lines = []\n",
    "    prev_y = None\n",
    "    cur_line = \"\"\n",
    "\n",
    "    for element in font_info:\n",
    "        cur_y = element['coordinates'][1]\n",
    "        if prev_y is None or cur_y == prev_y:\n",
    "            cur_line += \" \" + element['text']\n",
    "        else:\n",
    "            if cur_line.strip():\n",
    "                lines.append(cur_line.strip())\n",
    "            cur_line = element['text']\n",
    "        prev_y = cur_y\n",
    "\n",
    "    # Don't forget the last line\n",
    "    if cur_line.strip():\n",
    "        lines.append(cur_line.strip())\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            # patterns to filter out unwanted lines\n",
    "            'numbered_lines': re.compile(r'^\\d+\\.\\d+\\b'),\n",
    "            'symbol_only': re.compile(r'^[\\W_]+$'),\n",
    "            'copyright_pattern': re.compile(r'(©|ⓒ|\\(c\\)|\\(C\\)|c\\s*⃝)', re.IGNORECASE),\n",
    "            'exercises_pattern': re.compile(r'^\\s*Exercises?\\b[\\s\\d.:!?-]*$', re.IGNORECASE),\n",
    "            # noise patterns\n",
    "            'dotted_noise': re.compile(r'(?<!\\w)([.\\s]){3,}(?!\\w)'),  \n",
    "            'symbol_noise': re.compile(r'(?<!\\w)([\\W]\\s?){3,}(?!\\w)')\n",
    "            }\n",
    "\n",
    "    def filter_lines(self, lines):\n",
    "        \"\"\"Remove unwanted lines while keeping the structure\"\"\"\n",
    "        return [\n",
    "            line for line in lines\n",
    "            if not (self.patterns['numbered_lines'].match(line.strip()) or \n",
    "                   self.patterns['symbol_only'].match(line.strip()) or\n",
    "                   self.patterns['copyright_pattern'].search(line.strip()) or\n",
    "                   self.patterns['exercises_pattern'].match(line.strip())) \n",
    "        ]\n",
    "\n",
    "    def filter_noise(self, lines):\n",
    "        \"\"\"Remove noise patterns from lines\"\"\"\n",
    "        cleaned = []\n",
    "        for line in lines:\n",
    "            # Remove standalone noise sequences (not between words)\n",
    "            line = self.patterns['dotted_noise'].sub('', line)\n",
    "            line = self.patterns['symbol_noise'].sub('', line)\n",
    "            cleaned.append(line.strip())\n",
    "        return cleaned\n",
    "    \n",
    "    def process(self, lines):\n",
    "        \"\"\"Complete processing pipeline\"\"\"\n",
    "        filtered = self.filter_lines(lines)\n",
    "        cleaned = self.filter_noise(filtered)\n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08507f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "Foreword 1\n",
      "Part I Mathematical Foundations 9\n",
      "1 Introduction and Motivation 11\n",
      "2 Linear Algebra 17\n",
      "3 Analytic Geometry 70\n",
      "4 Matrix Decompositions 98\n",
      "i\n",
      "This material will be published by Cambridge University Press as  Mathematics for Machine Learn-\n",
      "ii Contents\n",
      "5 Vector Calculus 139\n",
      "6 Probability and Distributions 172\n",
      "7 Continuous Optimization 225\n",
      "Part II Central Machine Learning Problems 249\n",
      "8 When Models Meet Data 251\n",
      "Draft (2019-12-11) of “Mathematics for Machine Learning”. Feedback:  https://mml-book.com .\n",
      "Contents iii\n",
      "9 Linear Regression 289\n",
      "10 Dimensionality Reduction with Principal Component Analysis 317\n",
      "11 Density Estimation with Gaussian Mixture Models 348\n",
      "12 Classiﬁcation with Support Vector Machines 370\n",
      "References 395\n",
      "Index 407\n",
      "c\n",
      "⃝ 2019 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.\n"
     ]
    }
   ],
   "source": [
    "# font_info = extract_font_info(examples[key])\n",
    "# lines = extract_lines_from_font_info(font_info)\n",
    "# cleaner = TextCleaner()\n",
    "# processed_lines = cleaner.process(lines)\n",
    "\n",
    "# for line in processed_lines:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712811b",
   "metadata": {},
   "source": [
    "### Test runpod output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b130241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env from project root\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"RUNPOD_API_KEY\")\n",
    "ENDPOINT = os.getenv(\"RUNPOD_ENDPOINT\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25f50659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt: str, max_tokens: int = 256, context_length: int = 8192) -> str:\n",
    "    \"\"\"Submit a prompt to the RunPod endpoint and get back a response string.\"\"\"\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"prompt\": prompt,\n",
    "            \"options\": {\n",
    "                \"num_ctx\": context_length,      # Context window size\n",
    "                \"num_predict\": max_tokens       # Max tokens to generate\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Start job\n",
    "    response = requests.post(f\"{ENDPOINT}/run\", headers=HEADERS, json=payload)\n",
    "    job_id = response.json().get(\"id\")\n",
    "    print(f\"[RunPod] Job started: {job_id}\")\n",
    "\n",
    "    # Poll for status\n",
    "    while True:\n",
    "        status_res = requests.get(f\"{ENDPOINT}/status/{job_id}\", headers=HEADERS).json()\n",
    "        status = status_res.get(\"status\")\n",
    "        print(f\"[RunPod] Status: {status}\")\n",
    "        if status in (\"COMPLETED\", \"FAILED\"):\n",
    "            break\n",
    "        time.sleep(3)\n",
    "\n",
    "    if status == \"COMPLETED\":\n",
    "        return status_res[\"output\"][\"response\"]\n",
    "    else:\n",
    "        raise RuntimeError(\"RunPod job failed.\")\n",
    "\n",
    "\n",
    "def clean_and_parse_json(raw_text: str):\n",
    "    \"\"\"Clean and parse model output into JSON.\"\"\"\n",
    "    cleaned = raw_text.strip().strip(\"```json\").strip(\"```\").strip(\"'\")\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # Handle escaped quotes\n",
    "            unescaped = codecs.decode(cleaned, 'unicode_escape')\n",
    "            return json.loads(unescaped)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Could not parse JSON output\") from e\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bad1325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toc_prompt_new(toc_text: str):\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "You are a precise document parser. Extract main chapters from this table of contents.\n",
    "\n",
    "RULES:\n",
    "1. Extract ONLY numbered chapters (1, 2, 3...), NOT subsections (1.1, 1.2...)\n",
    "2. Use EXACT titles and page numbers from the document\n",
    "3. Return ONLY valid JSON array, no explanations\n",
    "4. If unclear, return empty array []\n",
    "\n",
    "Expected format:\n",
    "[{{\"chapter_number\": \"1\", \"chapter_title\": \"Introduction\", \"start_page\": 1, \"end_page\": 25}}]\n",
    "\n",
    "Table of contents:\n",
    "{toc_text}\n",
    "\n",
    "Return JSON only:<end_of_turn>\n",
    "<start_of_turn>model\n",
    "[\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def toc_prompt2(toc_text: str):\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "You are a precise document parser that extracts structured information from table of contents. You NEVER hallucinate, invent, or make up information. You ONLY extract what is explicitly present in the provided text.\n",
    "\n",
    "I need to extract main chapter information from this table of contents. Only extract numbered chapters, ignore subsections.\n",
    "\n",
    "Here is the table of contents:\n",
    "{toc_text}\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Extract ONLY main chapters that start with a number (1, 2, 3, etc.)\n",
    "2. Do NOT extract subsections (like 1.1, 1.2, 2.1, etc.)\n",
    "3. Use the EXACT chapter titles shown in the document\n",
    "4. Use the EXACT page numbers shown in the document. The starting page of a chapter is the page where the chapter starts, and the end page is the page before the next chapter starts.\n",
    "5. Handle both roman numerals (i, ii, iii, v, x) and arabic numerals (1, 25, 100)\n",
    "6. Calculate end pages as: next chapter's start page minus 1\n",
    "7. Return ONLY valid JSON - no explanations, no markdown formatting\n",
    "8. If you cannot clearly identify chapters, return empty array []\n",
    "9. Exclude chapters starting with 0\n",
    "\n",
    "Return JSON array: [{{\"chapter_number\": \"X\", \"chapter_title\": \"...\", \"start_page\": X, \"end_page\": X}}]<end_of_turn>\n",
    "<start_of_turn>model\n",
    "I will carefully examine the table of contents and extract only the main chapters that are explicitly shown, using their exact titles and page numbers.\n",
    "\n",
    "Looking at the provided table of contents, I can identify the following main chapters:\n",
    "\n",
    "[<end_of_turn>\n",
    "<start_of_turn>user\n",
    "Continue with the complete JSON array.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def toc_prompt3(toc_text: str):\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "You are a precise document parser that extracts structured information from table of contents. You NEVER hallucinate, invent, or make up information. You ONLY extract what is explicitly present in the provided text.\n",
    "\n",
    "I need to extract main chapter information from this table of contents. Only extract numbered chapters, ignore subsections.\n",
    "\n",
    "Here is the table of contents:\n",
    "{toc_text}\n",
    "\n",
    "CRITICAL RULES:\n",
    "CRITICAL RULES:\n",
    "1. Extract ONLY main chapters that start with a number (1, 2, 3, etc.)\n",
    "2. Do NOT extract subsections (like 1.1, 1.2, 2.1, etc.)\n",
    "3. Use the EXACT chapter titles shown in the document\n",
    "4. Page numbers MUST be calculated as follows:\n",
    "   - The starting page is ALWAYS the exact page number shown for the chapter\n",
    "   - The end page is ALWAYS calculated as: (next chapter's start page) minus 1\n",
    "   - If there is no next chapter, leave end_page as null\n",
    "5. Handle both roman numerals (i, ii, iii, v, x) and arabic numerals (1, 25, 100)\n",
    "6. Return ONLY valid JSON - no explanations, no markdown formatting\n",
    "7. If you cannot clearly identify chapters, return empty array []\n",
    "8. Exclude chapters starting with 0\n",
    "9. DOUBLE-CHECK your end_page calculations based on the next chapter's start page\n",
    "\n",
    "Return JSON array in this exact format:\n",
    "[{{\"chapter_number\": \"X\", \"chapter_title\": \"...\", \"start_page\": X, \"end_page\": X|null}}]<end_of_turn>\n",
    "<start_of_turn>model\n",
    "I will carefully examine the table of contents and extract only the main chapters that are explicitly shown, using their exact titles and page numbers. I will calculate end pages precisely as (next chapter's start page) minus 1.\n",
    "\n",
    "Looking at the provided table of contents, I can identify the following main chapters:\n",
    "\n",
    "[<end_of_turn>\n",
    "<start_of_turn>user\n",
    "Continue with the complete JSON array, ensuring end_page is always calculated correctly based on the next chapter's start page.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fc6d4765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "About the Author\n",
      "xxi\n",
      "PREAMBLE\n",
      "1\n",
      "1\n",
      "Financial Machine Learning as a Distinct Subject\n",
      "3\n",
      "1.1\n",
      "Motivation, 3\n",
      "1.2\n",
      "The Main Reason Financial Machine Learning Projects Usually Fail, 4\n",
      "1.2.1\n",
      "The Sisyphus Paradigm, 4\n",
      "1.2.2\n",
      "The Meta-Strategy Paradigm, 5\n",
      "1.3\n",
      "Book Structure, 6\n",
      "1.3.1\n",
      "Structure by Production Chain, 6\n",
      "1.3.2\n",
      "Structure by Strategy Component, 9\n",
      "1.3.3\n",
      "Structure by Common Pitfall, 12\n",
      "1.4\n",
      "Target Audience, 12\n",
      "1.5\n",
      "Requisites, 13\n",
      "1.6\n",
      "FAQs, 14\n",
      "1.7\n",
      "Acknowledgments, 18\n",
      "Exercises, 19\n",
      "References, 20\n",
      "Bibliography, 20\n",
      "PART 1\n",
      "DATA ANALYSIS\n",
      "21\n",
      "2\n",
      "Financial Data Structures\n",
      "23\n",
      "2.1\n",
      "Motivation, 23\n",
      "ix\n",
      "\n",
      "x\n",
      "CONTENTS\n",
      "2.2\n",
      "Essential Types of Financial Data, 23\n",
      "2.2.1\n",
      "Fundamental Data, 23\n",
      "2.2.2\n",
      "Market Data, 24\n",
      "2.2.3\n",
      "Analytics, 25\n",
      "2.2.4\n",
      "Alternative Data, 25\n",
      "2.3\n",
      "Bars, 25\n",
      "2.3.1\n",
      "Standard Bars, 26\n",
      "2.3.2\n",
      "Information-Driven Bars, 29\n",
      "2.4\n",
      "Dealing with Multi-Product Series, 32\n",
      "2.4.1\n",
      "The ETF Trick, 33\n",
      "2.4.2\n",
      "PCA Weights, 35\n",
      "2.4.3\n",
      "Single Future Roll, 36\n",
      "2.5\n",
      "Sampling Features, 38\n",
      "2.5.1\n",
      "Sampling for Reduction, 38\n",
      "2.5.2\n",
      "Event-Based Sampling, 38\n",
      "Exercises, 40\n",
      "References, 41\n",
      "3\n",
      "Labeling\n",
      "43\n",
      "3.1\n",
      "Motivation, 43\n",
      "3.2\n",
      "The Fixed-Time Horizon Method, 43\n",
      "3.3\n",
      "Computing Dynamic Thresholds, 44\n",
      "3.4\n",
      "The Triple-Barrier Method, 45\n",
      "3.5\n",
      "Learning Side and Size, 48\n",
      "3.6\n",
      "Meta-Labeling, 50\n",
      "3.7\n",
      "How to Use Meta-Labeling, 51\n",
      "3.8\n",
      "The Quantamental Way, 53\n",
      "3.9\n",
      "Dropping Unnecessary Labels, 54\n",
      "Exercises, 55\n",
      "Bibliography, 56\n",
      "4\n",
      "Sample Weights\n",
      "59\n",
      "4.1\n",
      "Motivation, 59\n",
      "4.2\n",
      "Overlapping Outcomes, 59\n",
      "4.3\n",
      "Number of Concurrent Labels, 60\n",
      "4.4\n",
      "Average Uniqueness of a Label, 61\n",
      "4.5\n",
      "Bagging Classifiers and Uniqueness, 62\n",
      "4.5.1\n",
      "Sequential Bootstrap, 63\n",
      "4.5.2\n",
      "Implementation of Sequential Bootstrap, 64\n",
      "\n",
      "CONTENTS\n",
      "xi\n",
      "4.5.3\n",
      "A Numerical Example, 65\n",
      "4.5.4\n",
      "Monte Carlo Experiments, 66\n",
      "4.6\n",
      "Return Attribution, 68\n",
      "4.7\n",
      "Time Decay, 70\n",
      "4.8\n",
      "Class Weights, 71\n",
      "Exercises, 72\n",
      "References, 73\n",
      "Bibliography, 73\n",
      "5\n",
      "Fractionally Differentiated Features\n",
      "75\n",
      "5.1\n",
      "Motivation, 75\n",
      "5.2\n",
      "The Stationarity vs. Memory Dilemma, 75\n",
      "5.3\n",
      "Literature Review, 76\n",
      "5.4\n",
      "The Method, 77\n",
      "5.4.1\n",
      "Long Memory, 77\n",
      "5.4.2\n",
      "Iterative Estimation, 78\n",
      "5.4.3\n",
      "Convergence, 80\n",
      "5.5\n",
      "Implementation, 80\n",
      "5.5.1\n",
      "Expanding Window, 80\n",
      "5.5.2\n",
      "Fixed-Width Window Fracdiff, 82\n",
      "5.6\n",
      "Stationarity with Maximum Memory Preservation, 84\n",
      "5.7\n",
      "Conclusion, 88\n",
      "Exercises, 88\n",
      "References, 89\n",
      "Bibliography, 89\n",
      "PART 2\n",
      "MODELLING\n",
      "91\n",
      "6\n",
      "Ensemble Methods\n",
      "93\n",
      "6.1\n",
      "Motivation, 93\n",
      "6.2\n",
      "The Three Sources of Errors, 93\n",
      "6.3\n",
      "Bootstrap Aggregation, 94\n",
      "6.3.1\n",
      "Variance Reduction, 94\n",
      "6.3.2\n",
      "Improved Accuracy, 96\n",
      "6.3.3\n",
      "Observation Redundancy, 97\n",
      "6.4\n",
      "Random Forest, 98\n",
      "6.5\n",
      "Boosting, 99\n",
      "\n",
      "xii\n",
      "CONTENTS\n",
      "6.6\n",
      "Bagging vs. Boosting in Finance, 100\n",
      "6.7\n",
      "Bagging for Scalability, 101\n",
      "Exercises, 101\n",
      "References, 102\n",
      "Bibliography, 102\n",
      "7\n",
      "Cross-Validation in Finance\n",
      "103\n",
      "7.1\n",
      "Motivation, 103\n",
      "7.2\n",
      "The Goal of Cross-Validation, 103\n",
      "7.3\n",
      "Why K-Fold CV Fails in Finance, 104\n",
      "7.4\n",
      "A Solution: Purged K-Fold CV, 105\n",
      "7.4.1\n",
      "Purging the Training Set, 105\n",
      "7.4.2\n",
      "Embargo, 107\n",
      "7.4.3\n",
      "The Purged K-Fold Class, 108\n",
      "7.5\n",
      "Bugs in Sklearn’s Cross-Validation, 109\n",
      "Exercises, 110\n",
      "Bibliography, 111\n",
      "8\n",
      "Feature Importance\n",
      "113\n",
      "8.1\n",
      "Motivation, 113\n",
      "8.2\n",
      "The Importance of Feature Importance, 113\n",
      "8.3\n",
      "Feature Importance with Substitution Effects, 114\n",
      "8.3.1\n",
      "Mean Decrease Impurity, 114\n",
      "8.3.2\n",
      "Mean Decrease Accuracy, 116\n",
      "8.4\n",
      "Feature Importance without Substitution Effects, 117\n",
      "8.4.1\n",
      "Single Feature Importance, 117\n",
      "8.4.2\n",
      "Orthogonal Features, 118\n",
      "8.5\n",
      "Parallelized vs. Stacked Feature Importance, 121\n",
      "8.6\n",
      "Experiments with Synthetic Data, 122\n",
      "Exercises, 127\n",
      "References, 127\n",
      "9\n",
      "Hyper-Parameter Tuning with Cross-Validation\n",
      "129\n",
      "9.1\n",
      "Motivation, 129\n",
      "9.2\n",
      "Grid Search Cross-Validation, 129\n",
      "9.3\n",
      "Randomized Search Cross-Validation, 131\n",
      "9.3.1\n",
      "Log-Uniform Distribution, 132\n",
      "9.4\n",
      "Scoring and Hyper-parameter Tuning, 134\n",
      "\n",
      "CONTENTS\n",
      "xiii\n",
      "Exercises, 135\n",
      "References, 136\n",
      "Bibliography, 137\n",
      "PART 3\n",
      "BACKTESTING\n",
      "139\n",
      "10\n",
      "Bet Sizing\n",
      "141\n",
      "10.1\n",
      "Motivation, 141\n",
      "10.2\n",
      "Strategy-Independent Bet Sizing Approaches, 141\n",
      "10.3\n",
      "Bet Sizing from Predicted Probabilities, 142\n",
      "10.4\n",
      "Averaging Active Bets, 144\n",
      "10.5\n",
      "Size Discretization, 144\n",
      "10.6\n",
      "Dynamic Bet Sizes and Limit Prices, 145\n",
      "Exercises, 148\n",
      "References, 149\n",
      "Bibliography, 149\n",
      "11\n",
      "The Dangers of Backtesting\n",
      "151\n",
      "11.1\n",
      "Motivation, 151\n",
      "11.2\n",
      "Mission Impossible: The Flawless Backtest, 151\n",
      "11.3\n",
      "Even If Your Backtest Is Flawless, It Is Probably Wrong, 152\n",
      "11.4\n",
      "Backtesting Is Not a Research Tool, 153\n",
      "11.5\n",
      "A Few General Recommendations, 153\n",
      "11.6\n",
      "Strategy Selection, 155\n",
      "Exercises, 158\n",
      "References, 158\n",
      "Bibliography, 159\n",
      "12\n",
      "Backtesting through Cross-Validation\n",
      "161\n",
      "12.1\n",
      "Motivation, 161\n",
      "12.2\n",
      "The Walk-Forward Method, 161\n",
      "12.2.1\n",
      "Pitfalls of the Walk-Forward Method, 162\n",
      "12.3\n",
      "The Cross-Validation Method, 162\n",
      "12.4\n",
      "The Combinatorial Purged Cross-Validation Method, 163\n",
      "12.4.1\n",
      "Combinatorial Splits, 164\n",
      "12.4.2\n",
      "The Combinatorial Purged Cross-Validation\n",
      "Backtesting Algorithm, 165\n",
      "12.4.3\n",
      "A Few Examples, 165\n",
      "\n",
      "xiv\n",
      "CONTENTS\n",
      "12.5\n",
      "How Combinatorial Purged Cross-Validation Addresses\n",
      "Backtest Overfitting, 166\n",
      "Exercises, 167\n",
      "References, 168\n",
      "13\n",
      "Backtesting on Synthetic Data\n",
      "169\n",
      "13.1\n",
      "Motivation, 169\n",
      "13.2\n",
      "Trading Rules, 169\n",
      "13.3\n",
      "The Problem, 170\n",
      "13.4\n",
      "Our Framework, 172\n",
      "13.5\n",
      "Numerical Determination of Optimal Trading Rules, 173\n",
      "13.5.1\n",
      "The Algorithm, 173\n",
      "13.5.2\n",
      "Implementation, 174\n",
      "13.6\n",
      "Experimental Results, 176\n",
      "13.6.1\n",
      "Cases with Zero Long-Run Equilibrium, 177\n",
      "13.6.2\n",
      "Cases with Positive Long-Run Equilibrium, 180\n",
      "13.6.3\n",
      "Cases with Negative Long-Run Equilibrium, 182\n",
      "13.7\n",
      "Conclusion, 192\n",
      "Exercises, 192\n",
      "References, 193\n",
      "14\n",
      "Backtest Statistics\n",
      "195\n",
      "14.1\n",
      "Motivation, 195\n",
      "14.2\n",
      "Types of Backtest Statistics, 195\n",
      "14.3\n",
      "General Characteristics, 196\n",
      "14.4\n",
      "Performance, 198\n",
      "14.4.1\n",
      "Time-Weighted Rate of Return, 198\n",
      "14.5\n",
      "Runs, 199\n",
      "14.5.1\n",
      "Returns Concentration, 199\n",
      "14.5.2\n",
      "Drawdown and Time under Water, 201\n",
      "14.5.3\n",
      "Runs Statistics for Performance Evaluation, 201\n",
      "14.6\n",
      "Implementation Shortfall, 202\n",
      "14.7\n",
      "Efficiency, 203\n",
      "14.7.1\n",
      "The Sharpe Ratio, 203\n",
      "14.7.2\n",
      "The Probabilistic Sharpe Ratio, 203\n",
      "14.7.3\n",
      "The Deflated Sharpe Ratio, 204\n",
      "14.7.4\n",
      "Efficiency Statistics, 205\n",
      "14.8\n",
      "Classification Scores, 206\n",
      "14.9\n",
      "Attribution, 207\n",
      "\n",
      "CONTENTS\n",
      "xv\n",
      "Exercises, 208\n",
      "References, 209\n",
      "Bibliography, 209\n",
      "15\n",
      "Understanding Strategy Risk\n",
      "211\n",
      "15.1\n",
      "Motivation, 211\n",
      "15.2\n",
      "Symmetric Payouts, 211\n",
      "15.3\n",
      "Asymmetric Payouts, 213\n",
      "15.4\n",
      "The Probability of Strategy Failure, 216\n",
      "15.4.1\n",
      "Algorithm, 217\n",
      "15.4.2\n",
      "Implementation, 217\n",
      "Exercises, 219\n",
      "References, 220\n",
      "16\n",
      "Machine Learning Asset Allocation\n",
      "221\n",
      "16.1\n",
      "Motivation, 221\n",
      "16.2\n",
      "The Problem with Convex Portfolio Optimization, 221\n",
      "16.3\n",
      "Markowitz’s Curse, 222\n",
      "16.4\n",
      "From Geometric to Hierarchical Relationships, 223\n",
      "16.4.1\n",
      "Tree Clustering, 224\n",
      "16.4.2\n",
      "Quasi-Diagonalization, 229\n",
      "16.4.3\n",
      "Recursive Bisection, 229\n",
      "16.5\n",
      "A Numerical Example, 231\n",
      "16.6\n",
      "Out-of-Sample Monte Carlo Simulations, 234\n",
      "16.7\n",
      "Further Research, 236\n",
      "16.8\n",
      "Conclusion, 238\n",
      "Appendices, 239\n",
      "16.A.1\n",
      "Correlation-based Metric, 239\n",
      "16.A.2\n",
      "Inverse Variance Allocation, 239\n",
      "16.A.3\n",
      "Reproducing the Numerical Example, 240\n",
      "16.A.4\n",
      "Reproducing the Monte Carlo Experiment, 242\n",
      "Exercises, 244\n",
      "References, 245\n",
      "PART 4\n",
      "USEFUL FINANCIAL FEATURES\n",
      "247\n",
      "17\n",
      "Structural Breaks\n",
      "249\n",
      "17.1\n",
      "Motivation, 249\n",
      "17.2\n",
      "Types of Structural Break Tests, 249\n",
      "\n",
      "xvi\n",
      "CONTENTS\n",
      "17.3\n",
      "CUSUM Tests, 250\n",
      "17.3.1\n",
      "Brown-Durbin-Evans CUSUM Test on Recursive\n",
      "Residuals, 250\n",
      "17.3.2\n",
      "Chu-Stinchcombe-White CUSUM Test on Levels, 251\n",
      "17.4\n",
      "Explosiveness Tests, 251\n",
      "17.4.1\n",
      "Chow-Type Dickey-Fuller Test, 251\n",
      "17.4.2\n",
      "Supremum Augmented Dickey-Fuller, 252\n",
      "17.4.3\n",
      "Sub- and Super-Martingale Tests, 259\n",
      "Exercises, 261\n",
      "References, 261\n",
      "18\n",
      "Entropy Features\n",
      "263\n",
      "18.1\n",
      "Motivation, 263\n",
      "18.2\n",
      "Shannon’s Entropy, 263\n",
      "18.3\n",
      "The Plug-in (or Maximum Likelihood) Estimator, 264\n",
      "18.4\n",
      "Lempel-Ziv Estimators, 265\n",
      "18.5\n",
      "Encoding Schemes, 269\n",
      "18.5.1\n",
      "Binary Encoding, 270\n",
      "18.5.2\n",
      "Quantile Encoding, 270\n",
      "18.5.3\n",
      "Sigma Encoding, 270\n",
      "18.6\n",
      "Entropy of a Gaussian Process, 271\n",
      "18.7\n",
      "Entropy and the Generalized Mean, 271\n",
      "18.8\n",
      "A Few Financial Applications of Entropy, 275\n",
      "18.8.1\n",
      "Market Efficiency, 275\n",
      "18.8.2\n",
      "Maximum Entropy Generation, 275\n",
      "18.8.3\n",
      "Portfolio Concentration, 275\n",
      "18.8.4\n",
      "Market Microstructure, 276\n",
      "Exercises, 277\n",
      "References, 278\n",
      "Bibliography, 279\n",
      "19\n",
      "Microstructural Features\n",
      "281\n",
      "19.1\n",
      "Motivation, 281\n",
      "19.2\n",
      "Review of the Literature, 281\n",
      "19.3\n",
      "First Generation: Price Sequences, 282\n",
      "19.3.1\n",
      "The Tick Rule, 282\n",
      "19.3.2\n",
      "The Roll Model, 282\n",
      "\n",
      "CONTENTS\n",
      "xvii\n",
      "19.3.3\n",
      "High-Low Volatility Estimator, 283\n",
      "19.3.4\n",
      "Corwin and Schultz, 284\n",
      "19.4\n",
      "Second Generation: Strategic Trade Models, 286\n",
      "19.4.1\n",
      "Kyle’s Lambda, 286\n",
      "19.4.2\n",
      "Amihud’s Lambda, 288\n",
      "19.4.3\n",
      "Hasbrouck’s Lambda, 289\n",
      "19.5\n",
      "Third Generation: Sequential Trade Models, 290\n",
      "19.5.1\n",
      "Probability of Information-based Trading, 290\n",
      "19.5.2\n",
      "Volume-Synchronized Probability of Informed\n",
      "Trading, 292\n",
      "19.6\n",
      "Additional Features from Microstructural Datasets, 293\n",
      "19.6.1\n",
      "Distibution of Order Sizes, 293\n",
      "19.6.2\n",
      "Cancellation Rates, Limit Orders, Market Orders, 293\n",
      "19.6.3\n",
      "Time-Weighted Average Price Execution Algorithms, 294\n",
      "19.6.4\n",
      "Options Markets, 295\n",
      "19.6.5\n",
      "Serial Correlation of Signed Order Flow, 295\n",
      "19.7\n",
      "What Is Microstructural Information?, 295\n",
      "Exercises, 296\n",
      "References, 298\n",
      "PART 5\n",
      "HIGH-PERFORMANCE COMPUTING RECIPES\n",
      "301\n",
      "20\n",
      "Multiprocessing and Vectorization\n",
      "303\n",
      "20.1\n",
      "Motivation, 303\n",
      "20.2\n",
      "Vectorization Example, 303\n",
      "20.3\n",
      "Single-Thread vs. Multithreading vs. Multiprocessing, 304\n",
      "20.4\n",
      "Atoms and Molecules, 306\n",
      "20.4.1\n",
      "Linear Partitions, 306\n",
      "20.4.2\n",
      "Two-Nested Loops Partitions, 307\n",
      "20.5\n",
      "Multiprocessing Engines, 309\n",
      "20.5.1\n",
      "Preparing the Jobs, 309\n",
      "20.5.2\n",
      "Asynchronous Calls, 311\n",
      "20.5.3\n",
      "Unwrapping the Callback, 312\n",
      "20.5.4\n",
      "Pickle/Unpickle Objects, 313\n",
      "20.5.5\n",
      "Output Reduction, 313\n",
      "20.6\n",
      "Multiprocessing Example, 315\n",
      "Exercises, 316\n",
      "\n",
      "xviii\n",
      "CONTENTS\n",
      "Reference, 317\n",
      "Bibliography, 317\n",
      "21\n",
      "Brute Force and Quantum Computers\n",
      "319\n",
      "21.1\n",
      "Motivation, 319\n",
      "21.2\n",
      "Combinatorial Optimization, 319\n",
      "21.3\n",
      "The Objective Function, 320\n",
      "21.4\n",
      "The Problem, 321\n",
      "21.5\n",
      "An Integer Optimization Approach, 321\n",
      "21.5.1\n",
      "Pigeonhole Partitions, 321\n",
      "21.5.2\n",
      "Feasible Static Solutions, 323\n",
      "21.5.3\n",
      "Evaluating Trajectories, 323\n",
      "21.6\n",
      "A Numerical Example, 325\n",
      "21.6.1\n",
      "Random Matrices, 325\n",
      "21.6.2\n",
      "Static Solution, 326\n",
      "21.6.3\n",
      "Dynamic Solution, 327\n",
      "Exercises, 327\n",
      "References, 328\n",
      "22\n",
      "High-Performance Computational Intelligence and Forecasting\n",
      "Technologies\n",
      "329\n",
      "Kesheng Wu and Horst D. Simon\n",
      "22.1\n",
      "Motivation, 329\n",
      "22.2\n",
      "Regulatory Response to the Flash Crash of 2010, 329\n",
      "22.3\n",
      "Background, 330\n",
      "22.4\n",
      "HPC Hardware, 331\n",
      "22.5\n",
      "HPC Software, 335\n",
      "22.5.1\n",
      "Message Passing Interface, 335\n",
      "22.5.2\n",
      "Hierarchical Data Format 5, 336\n",
      "22.5.3\n",
      "In Situ Processing, 336\n",
      "22.5.4\n",
      "Convergence, 337\n",
      "22.6\n",
      "Use Cases, 337\n",
      "22.6.1\n",
      "Supernova Hunting, 337\n",
      "22.6.2\n",
      "Blobs in Fusion Plasma, 338\n",
      "22.6.3\n",
      "Intraday Peak Electricity Usage, 340\n",
      "22.6.4\n",
      "The Flash Crash of 2010, 341\n",
      "22.6.5\n",
      "Volume-synchronized Probability of Informed Trading\n",
      "Calibration, 346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping example keys to PDF paths\n",
    "examples = {\n",
    "    \"pdf_path1\": \"../data/mcelreath_2020_statistical-rethinking.pdf\",\n",
    "    \"pdf_path2\": \"../data/Theory of Statistic.pdf\",\n",
    "    \"pdf_path3\": \"../data/Deep Learning with Python.pdf\",\n",
    "    \"pdf_path4\": \"../data/Natural_Image_Statistics.pdf\",\n",
    "    \"pdf_path5\": \"../data/mml-book.pdf\",\n",
    "    \"pdf_path6\": \"../data/Python-Testing-with-pytest-Simple,-Rapid,-Effective,-and-Scalable,-2nd-Edition-by-Brian-Okken_bibis.ir.pdf\",\n",
    "    \"pdf_path7\": \"../data/Marcos Lopez de Prado - Advances in Financial Machine Learning-Wiley (2018).pdf\",\n",
    "    \"pdf_path8\": \"../data/finetuning.pdf\",\n",
    "}\n",
    "\n",
    "# Dictionary mapping example keys to page ranges to extract content from\n",
    "content_page_ranges = {\n",
    "    \"pdf_path1\": range(5, 8),\n",
    "    \"pdf_path2\": range(10, 17),\n",
    "    \"pdf_path3\": range(7, 13),\n",
    "    \"pdf_path4\": range(4, 13),\n",
    "    \"pdf_path5\": range(2, 5),\n",
    "    \"pdf_path6\": range(6, 10),\n",
    "    \"pdf_path7\": range(13, 23),\n",
    "    \"pdf_path8\": range(4, 9),\n",
    "}\n",
    "\n",
    "# Select example number\n",
    "n_example = 7\n",
    "key = f\"pdf_path{n_example}\"\n",
    "\n",
    "# Open the PDF\n",
    "doc = fitz.open(examples[key])\n",
    "\n",
    "# Extract text from the specified page range\n",
    "chapters_content_list = []\n",
    "for page_num in content_page_ranges[key]:\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text(\"text\")\n",
    "    chapters_content_list.append(text)\n",
    "\n",
    "# Join all text pages into a single string if needed\n",
    "chapters_content = \"\\n\".join(chapters_content_list)\n",
    "\n",
    "print(chapters_content)  # or pass it to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "14a13b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "About the Author xxi\n",
      "PREAMBLE 1\n",
      "1 Financial Machine Learning as a Distinct Subject 3\n",
      "Exercises ,  19\n",
      "References ,  20\n",
      "Bibliography ,  20\n",
      "PART 1 DATA ANALYSIS 21\n",
      "Exercises ,  40\n",
      "References ,  41\n",
      "3 Labeling 43\n",
      "Exercises ,  55\n",
      "Bibliography ,  56\n",
      "4 Sample Weights 59\n",
      "Exercises ,  72\n",
      "References ,  73\n",
      "Bibliography ,  73\n",
      "5 Fractionally Differentiated Features 75\n",
      "Exercises ,  88\n",
      "References ,  89\n",
      "Bibliography ,  89\n",
      "PART 2 MODELLING 91\n",
      "6 Ensemble Methods 93\n",
      "Exercises ,  101\n",
      "References ,  102\n",
      "Bibliography ,  102\n",
      "7 Cross-Validation in Finance 103\n",
      "Exercises ,  110\n",
      "Bibliography ,  111\n",
      "8 Feature Importance 113\n",
      "Exercises ,  127\n",
      "References ,  127\n",
      "9 Hyper-Parameter Tuning with Cross-Validation 129\n",
      "Exercises ,  135\n",
      "References ,  136\n",
      "Bibliography ,  137\n",
      "PART 3 BACKTESTING 139\n",
      "10 Bet Sizing 141\n",
      "Exercises ,  148\n",
      "References ,  149\n",
      "Bibliography ,  149\n",
      "11 The Dangers of Backtesting 151\n",
      "Exercises ,  158\n",
      "References ,  158\n",
      "Bibliography ,  159\n",
      "12 Backtesting through Cross-Validation 161\n",
      "Backtest Overfitting ,  166\n",
      "Exercises ,  167\n",
      "References ,  168\n",
      "13 Backtesting on Synthetic Data 169\n",
      "Exercises ,  192\n",
      "References ,  193\n",
      "14 Backtest Statistics 195\n",
      "Exercises ,  208\n",
      "References ,  209\n",
      "Bibliography ,  209\n",
      "15 Understanding Strategy Risk 211\n",
      "Exercises ,  219\n",
      "References ,  220\n",
      "16 Machine Learning Asset Allocation 221\n",
      "Appendices ,  239\n",
      "16.A.1 Correlation-based Metric ,  239\n",
      "16.A.2 Inverse Variance Allocation ,  239\n",
      "16.A.3 Reproducing the Numerical Example ,  240\n",
      "16.A.4 Reproducing the Monte Carlo Experiment ,  242\n",
      "Exercises ,  244\n",
      "References ,  245\n",
      "Residuals ,  250\n",
      "Exercises ,  261\n",
      "References ,  261\n",
      "18 Entropy Features 263\n",
      "Exercises ,  277\n",
      "References ,  278\n",
      "Bibliography ,  279\n",
      "19 Microstructural Features 281\n",
      "Trading ,  292\n",
      "Exercises ,  296\n",
      "References ,  298\n",
      "PART 5 HIGH-PERFORMANCE COMPUTING RECIPES 301\n",
      "20 Multiprocessing and Vectorization 303\n",
      "Reference ,  317\n",
      "Bibliography ,  317\n",
      "21 Brute Force and Quantum Computers 319\n",
      "Exercises ,  327\n",
      "References ,  328\n",
      "22 High-Performance Computational Intelligence and Forecasting\n",
      "Technologies 329\n",
      "Kesheng Wu and Horst D. Simon\n"
     ]
    }
   ],
   "source": [
    "font_info = extract_font_info(examples[key])\n",
    "lines = extract_lines_from_font_info(font_info)\n",
    "cleaner = TextCleaner()\n",
    "processed_lines = cleaner.process(lines)\n",
    "\n",
    "for line in processed_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d2a3197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RunPod] Job started: 2d17d148-4689-4963-ae76-cf58f3039a00-e2\n",
      "[RunPod] Status: IN_QUEUE\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# steb by step call\n",
    "processed_lines_str = \"\\n\".join(processed_lines)\n",
    "toc_prompt_text = toc_prompt3(processed_lines_str)\n",
    "raw_output = result = run_prompt(\n",
    "    prompt=toc_prompt_text,\n",
    "    max_tokens=1024, #1024,        # Enough for JSON array\n",
    "    context_length=10000 #16384    # Large context for long TOC\n",
    ")\n",
    "json_output = clean_and_parse_json(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "46bb0146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chapter_number': '1',\n",
       "  'chapter_title': 'Financial Machine Learning as a Distinct Subject',\n",
       "  'start_page': 3,\n",
       "  'end_page': 19},\n",
       " {'chapter_number': '3',\n",
       "  'chapter_title': 'Labeling',\n",
       "  'start_page': 43,\n",
       "  'end_page': 55},\n",
       " {'chapter_number': '4',\n",
       "  'chapter_title': 'Sample Weights',\n",
       "  'start_page': 59,\n",
       "  'end_page': 72},\n",
       " {'chapter_number': '5',\n",
       "  'chapter_title': 'Fractionally Differentiated Features',\n",
       "  'start_page': 75,\n",
       "  'end_page': 88},\n",
       " {'chapter_number': '6',\n",
       "  'chapter_title': 'Ensemble Methods',\n",
       "  'start_page': 93,\n",
       "  'end_page': 101},\n",
       " {'chapter_number': '7',\n",
       "  'chapter_title': 'Cross-Validation in Finance',\n",
       "  'start_page': 103,\n",
       "  'end_page': 110},\n",
       " {'chapter_number': '8',\n",
       "  'chapter_title': 'Feature Importance',\n",
       "  'start_page': 113,\n",
       "  'end_page': 127},\n",
       " {'chapter_number': '9',\n",
       "  'chapter_title': 'Hyper-Parameter Tuning with Cross-Validation',\n",
       "  'start_page': 129,\n",
       "  'end_page': 135},\n",
       " {'chapter_number': '10',\n",
       "  'chapter_title': 'Bet Sizing',\n",
       "  'start_page': 141,\n",
       "  'end_page': 148},\n",
       " {'chapter_number': '11',\n",
       "  'chapter_title': 'The Dangers of Backtesting',\n",
       "  'start_page': 151,\n",
       "  'end_page': 158},\n",
       " {'chapter_number': '12',\n",
       "  'chapter_title': 'Backtesting through Cross-Validation',\n",
       "  'start_page': 161,\n",
       "  'end_page': 167},\n",
       " {'chapter_number': '13',\n",
       "  'chapter_title': 'Backtesting on Synthetic Data',\n",
       "  'start_page': 169,\n",
       "  'end_page': 192},\n",
       " {'chapter_number': '14',\n",
       "  'chapter_title': 'Backtest Statistics',\n",
       "  'start_page': 195,\n",
       "  'end_page': 208},\n",
       " {'chapter_number': '15',\n",
       "  'chapter_title': 'Understanding Strategy Risk',\n",
       "  'start_page': 211,\n",
       "  'end_page': 219},\n",
       " {'chapter_number': '16',\n",
       "  'chapter_title': 'Machine Learning Asset Allocation',\n",
       "  'start_page': 221,\n",
       "  'end_page': 244},\n",
       " {'chapter_number': '18',\n",
       "  'chapter_title': 'Entropy Features',\n",
       "  'start_page': 263,\n",
       "  'end_page': 277},\n",
       " {'chapter_number': '19',\n",
       "  'chapter_title': 'Microstructural Features',\n",
       "  'start_page': 281,\n",
       "  'end_page': 296},\n",
       " {'chapter_number': '20',\n",
       "  'chapter_title': 'Multiprocessing and Vectorization',\n",
       "  'start_page': 303,\n",
       "  'end_page': 317},\n",
       " {'chapter_number': '21',\n",
       "  'chapter_title': 'Brute Force and Quantum Computers',\n",
       "  'start_page': 319,\n",
       "  'end_page': 327},\n",
       " {'chapter_number': '22',\n",
       "  'chapter_title': 'High-Performance Computational Intelligence and Forecasting Technologies',\n",
       "  'start_page': 329,\n",
       "  'end_page': None}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811b93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
